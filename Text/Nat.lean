import VersoManual
import Content.Meta

open Verso.Genre Manual
open Verso.Genre.Manual.InlineLean
open Verso.Code.External

set_option verso.exampleProject "."
set_option verso.exampleModule "Content.NatProofs"

#doc (Manual) "The Natural Numbers" =>

We have already used the natural numbers (`ℕ`) in examples but now
we will formally define them. We will in spirit follow Guiseppe Peano who
codified the laws of natural numbers using predicate logic in the late
19th century. This is referred to as *Peano Arithmetic*.

![Giuseppe Peano (1858–1932)](diagrams/peano.jpg)
*Giuseppe Peano (1858–1932)*

Peano viewed the natural numbers as created from `zero` (`0 = zero`) and `succ`
(successor), i.e. `1 = succ 0`, `2 = succ 1` and so on. In Lean this corresponds to the following inductive definition:
```anchor NatDef
inductive Nat : Type
| zero : Nat
| succ : Nat → Nat
```
This declaration means:

* There is a new type `Nat : Type`,
* There are two elements `zero : Nat`, and given `n : Nat` we have `succ n : nat`,
* All the elements of `nat` can be generated by `zero` and then applying `succ` a finite number of times,
* `zero` and `succ n` are different elements,
* `succ` is injective, i.e. given `succ m = succ n` then we know that `m = n`.

We adopt the notation that `ℕ = Nat`, and Lean also automatically translates the usual decimal notation into elements of `ℕ`. This is convenient because otherwise it would be quite cumbersome to write out a number like `1234 : ℕ`.

# Basic properties of `ℕ`

## No confusion

Let's verify some basic properties of `ℕ` which actually correspond to some of Peano's axioms. First of all we want to verify that `0 ≠ succ n`, which corresponds to `true ≠ false` for `Bool` and indeed the same approach works here.
```anchor NoConf
example : ∀ n : ℕ , succ n ≠ zero := by
intro n h
cases h
```
I leave it as an exercise to establish this *no confusion property* from first principles as we have done for `Bool`.

## `succ` is injective

Next let's show that `succ` is injective. To do this we define a predecessor function `pred` using pattern matching, which works the same way as for `bool`:
```anchor pred
def pred : ℕ → ℕ
| zero => zero
| succ n => n
```
We can test the function using `#reduce (pred 7)`.

We defined `pred 0 = 0` which is a bit arbitrary. However, `pred` does the job to show that `succ` is injective:
```anchor succInj
example : ∀ m n : ℕ, succ m = succ n → m = n := by
intro m n h
change pred (succ m) = n
rw [h]
rfl
```
Here we use `change` to replace the goal `m = n` with `pred (succ m) = n` exploiting that `pred (succ x) = x`. On the new goal we can apply `rw`.

However, there is also a tactic called `injection` which does this automatically for all inductive types, avoiding the need to define `pred`:
```anchor succInj2
example : ∀ m n : ℕ, succ m = succ n → m = n := by
intro m n h
injection h
```

# Structural recursion

When defining functions on `ℕ` we will need recursion. Unlike the general recursion available in programming languages, we will only use *structural recursion*. That is, when we define a function on the natural numbers we can use the function on `n` to compute it for `succ n`. A simple example is the `double` function, which doubles a number:
```anchor double
def double : ℕ → ℕ
| zero => zero
| succ n => succ (succ (double n))
```
Here `double (succ n)` is `succ (succ (double n))`. That is, every `succ` is replaced by two `succ`s.

Because every natural number can be obtained by applying `succ` a finite number of times, recursive functions like this terminate in a finite number of steps. Non-terminating definitions, such as
```
def bad : ℕ → ℕ
| zero => zero
| succ n => succ (bad (succ n))
```
are rejected by Lean, since they don't define a mathematical function on `ℕ`.

We can define the inverse to `double` (division by 2 without remainder):
```anchor half
def half : ℕ → ℕ
| zero => zero
| succ zero => zero
| succ (succ n) => succ (half n)
```

Here `half` replaces every two `succ`s by one.

# Induction

Proof by induction is very closely related to structural recursion. It is basically the same idea, but for proofs. As an example, let's actually prove that `half` is the inverse of `double`:
```anchor halfDouble
theorem half_double :
  ∀ n : ℕ, half (double n) = n := by
intro n
induction n with
| zero => rfl
| succ n ih =>
    calc
      half (double (succ n))
      = half (succ (succ (double n))) := by rfl
      _ = succ (half (double n)) := by rfl
      _ = succ n := by rw [ih]
```
After `intro n` we are in the following state:

```
n : ℕ
⊢ half (double n) = n
```

`induction n` works very similarly to `cases`, but it gives us an extra assumption in the successor case—the *induction hypothesis*. That is in the `succ` case we have an extra argument (which I call `ih` for *induction hypothesis*).
```
m : ℕ
ih : half (double m) = m
⊢ half (double (m + 1)) = m + 1
```
This means that we can assume the statement for `m` when proving it for `succ m` (which is displayed as `m + 1`.)

We prove this in several steps using `calc`, starting with
```
half (double (succ n))
```
we apply the definition of `double` (by saying `rfl`):
```
= half (succ (succ (double n))) := by rfl
```
and now the definition of `half` (again using `rfl`)
```
_ = succ (half (double n)) := by rfl
```
The `_` is just an indicator for lean that we are chaining equations.
Now we have a term that matches the induction hypothesis and we can just rewrite:
```
_ = succ n := by rw [ih]
```
and we are done.

This is a very simple inductive proof, but it shows the general idea. Because every number is finitely generated from `zero` and `succ`, we can *run* an inductive proof for any number by repeating the inductive step as many times as there are `succ`s.

Induction is the main workhorse for proving properties of inductive types like `ℕ`. Try to fully understand the idea by looking at the simple proof above!

# Addition and its properties

While addition is an operation which you may have learned already in
kindergarten, it still needs to be defined. And, horror, its definition
already uses recursion — they don't tell the kids this in kindergarten!

Here is Lean's definition of addition :
```anchor add
def add : ℕ → ℕ → ℕ
| m  , zero     => m
| m  , (succ n) => succ (add m n)
```
`add m n` applies `n` successors to `m`. We define `m + n` as
`add m n`. For example:
```
3 + 2
= add 3 2
= add 3 (succ 1)
= succ (add 3 1)
= succ (add 3 (succ 0))
= succ (succ (add 3 0))
= succ (succ (add 3 zero))
= succ (succ 3)
= 5
```

Lean also introduces the usual notation that `m + n` is defined as `add m n`.

Lean defines addition by recursion over the second argument. While it
might seem more natural to recurse over the first argument, the result
is the same because addition is commutative (`m + n = n + m`), as we
will show soon. We will stick with Lean’s convention for now.

Now, what are the basic algebraic properties of `+`?

First of all, `0` is a *neutral element*, meaning `n + 0 = n` and
`0 + n = n`. You might think we only need to prove one of them since
addition is commutative, but we actually need both when proving
commutativity. It turns out that one is trivial, while the other
requires induction.

The theorem `add_rneutr` (`n + 0 = n`) holds by definition of `add`.
```anchor add_rneutr
theorem add_rneutr : ∀ n : ℕ, n + 0 = n := by
intro n
rfl
```

However, `add_lneutr` (`0 + n = n`) does require induction.
```anchor add_lneutr
theorem add_lneutr : ∀ n : ℕ, 0 + n  = n := by
intro n
induction n with
 | zero => rfl
 | succ m ih =>
     calc 0 + (succ m)
          = succ (0 + m)  := by rfl
          _ = succ m := by rw [ih]
```

This asymmetry comes from the definition of `+` (`add`) by recursion over the 2nd argument. If we had defined it by recursion of the first it would be the other way around.

Another important property of addition is that brackets don’t matter:
`(l + m) + n = l + (m + n)`. This is called *associativity*.
We again need induction, but the variable we choose for induction
matters. Since addition is defined by matching on its second argument,
we must perform induction on that argument (`n`).
```anchor add_assoc
theorem add_assoc : ∀ l m n : ℕ , (l + m) + n = l + (m + n) := by
intro l m n
induction n with
| zero => rfl
| succ n' ih =>
     calc  (l + m) + (succ n')
         = succ ((l + m) +n') := by rfl
      _  = succ (l + (m + n')) := by rw [ih]
      _  = l + (succ (m + n')) := by rfl
      _  = l + (m + (succ n')) := by rfl
```

In the base case (`n = 0`), both sides reduce to `l + m`.
In the successor case, we are able to push through `succ` on both sides (using jsut the definition of `+`) to be able to apply the induction hypothesis.

We have now shown the following facts about `+` and `0`:

* `0` is right neutral: `n + 0 = n` (`add_rneutr`),
* `0` is left neutral: `0 + n = n` (`add_lneutr`),
* `+` is associative: `(l + m) + n = l + (m + n)` (`add_assoc`).

Such a structure is called a *monoid*.
(If you look up "monoid" on Wikipedia, don’t be distracted by the
philosophical concept of the same name!)

However, we have not yet discussed *commutativity* (`l + m = m + l`) , which isn’t
required for a monoid.

To prove commutativity, it isn't clear on which variable to do induction because both appear in different positions. So we may as well do induction on the second (`m`) so that the left hand side reduces. The zero case can be proven using `l_neutr` which but the successor case is more difficult: our induction hypothesis is `ih : l + m = m + l` and we want to show `l + succ m = succ m + l`. We reason:
```
        l + (succ m)
           = succ (l + m) := by rfl
```
and now we can already apply the induction hypothesis:
```
         _ = succ (m + l) := by rw[ih]
```
but how do we bridge the gap to `succ (m + l)`? This is a common situation when proving theorems, which is a bit like using stepping stones to cross a stream. We need to put an additional stone in which in this case is a *lemma* (i.e. an auxilliary theorem), which can be proven by induction:
```anchor add_succ
theorem add_succ :
∀ l m : ℕ, (succ l) + m = succ (l + m) := by
intro l m
induction m with
 | zero => rfl
 | succ m ih =>
     calc
       (succ l) + (succ m)
         = succ ((succ l ) + m) := by rfl
       _ = succ (succ (l + m)) := by rw [ih]
       _ = succ (l + succ m) := by rfl
```
We can now use our *stepping stone* to prove commutativity:
```anchor add_comm
theorem add_comm :
∀ l m : ℕ, l + m = m + l := by
intro l m
induction m with
| zero =>
    calc l + 0
        = l := by rfl
      _ = 0 + l := by rw [add_lneutr]
| succ m ih =>
      calc   l + (succ m)
           = succ (l + m) := by rfl
         _ = succ (m + l) := by rw[ih]
         _ = (succ m) + l := by rw [← add_succ]
```

The lemmas we needed to prove commutativity of addition are just the mirror image of the equation which follow from the definition, that is by definition we have that
```
m + 0 = m
m + (succ n) = succ (m + n)
```
and for commutativity we need to establish
```
0 + m = m
(succ m) + n = succ (m + b)
```
Together with the previous results, we have now established that
`ℕ` with `+` and `0` forms a *commutative monoid*.

Mathematicians prefer it if you also have inverses as for the integers:
for every integer `i : ℤ` there is `−i : ℤ` such that `i + (−i) = 0` and `(−i) + i = 0`. Such a structure is called a *group*. Note that subtraction is now a derived operation `i - j = i + (-j)`.

# Multiplication and its properties

We are proceeding slowly, and this time we will leave more of the fun to you.
First, let’s define the idea of multiplication informally.

As `+` is repeated successor, `*` is repeated addition. That is,
`m*n` is `m` added `n` times. For example, `3*2` is `3 + 3 = 6`.
```anchor mult
def mul : ℕ → ℕ → ℕ
| _ , zero => zero
| m , (succ n) => mul m n + m
```

And as usual we define `x * y` to stand for `mul x y`. As `+`
was repeated `succ`, `*` is repeated `+`. That is `m * n` is
`m` added `n` times, for example::

```
  3 * 2
  = mul 3 2
  = mul 3 (succ 1)
  = mul 3 1 + 3
  = mul 3 (succ 0) + 3
  = mul 3 0 + 3 + 3
  = 0 + 3 + 3
  = 6
```

What are the properties of multiplication? First, it also forms a
commutative monoid, with `1` now playing the role of `0` for addition — it is
the neutral element for multiplication. The fundamental properties to prove are:



```anchor mult_cmon
theorem mult_rneutr : ∀ n : ℕ, n * 1 = n := by sorry
theorem mult_lneutr : ∀ n : ℕ, 1 * n  = n := by sorry
theorem mult_assoc : ∀ l m n : ℕ , (l * m) * n = l * (m * n) := by sorry
theorem mult_comm :  ∀ m n : ℕ , m * n = n * m := by sorry
```

Proving these will certainly require the properties of addition that we have
already established, and the order in which you prove them might differ from
how they are listed here. You may also want to keep the next properties
(distributivity and multiplication by zero) in mind as they interact with the
proofs above.

Apart from addition and multiplication both being commutative monoids,
they also interact in an important way known as *distributivity*.
For example, to simplify an expression of the form `x * (y + z)` you can
“multiply out” to get `(x * y) + (x * z)`. There are also boundary cases with
zero. Without assuming commutativity of multiplication, we want the symmetric
statements as well. The key properties are:

```anchor mult_distr
theorem mult_lzero : ∀ n : ℕ , 0 * n = 0 := by sorry
theorem mult_rzero : ∀ n : ℕ , n * 0 = 0 := by sorry
theorem mult_ldistr :  ∀ l m n : ℕ , (m + n) * l = m * l + n * l := sorry
theorem mult_rdistr :  ∀ l m n : ℕ , l * (m + n) = l * m + l * n := sorry
```

The structure consisting of two monoids together with these distributivity
laws (and with addition commutative) is called a *semiring*.
When multiplication is commutative as well, we have a *commutative semiring*.
Rings and semirings are central in algebra and closely related to polynomials
(expressions like `7*x^2 + x + 5`, possibly with higher exponents and several
variables).

# Some Algebra

Once we know that multiplication and addition form a commutative ring, we can
establish many familiar identities. For example, the binomial formula
`(x + y)^2 = x^2 + 2·x·y + y^2`. To state this, we define exponentiation as
repeated multiplication (just as multiplication is repeated addition).

In practice, proving polynomial identities by hand can be tedious. Lean's mathlib provides automation in form of a `ring` tactic
that can solve any equality that follows from the ring or semiring axioms by
normalising expressions to a canonical polynomial form and comparing them.
The important point is that the tactic still produces a proof from first
principles — there is no “cheating,” only verified calculation.

Here is a quick overview of standard number systems and their algebraic
structures:

- Natural numbers (`ℕ`): *Semiring*
- Integers (`ℤ`): *Ring*
- Rational numbers (`ℚ`): *Field*
- Real numbers (`ℝ`): *Complete field*
- Complex numbers (`ℂ`): *Algebraically complete field*

A *ring* is a semiring with *additive inverses*: for every `x : ℤ` there is
`-x : ℤ` such that `x + (-x) = 0` and `(-x) + x = 0`. Subtraction is not
primitive but is defined by adding the additive inverse: `x - y := x + (-y)`.

A *field* also has *multiplicative inverses* for all numbers different from
`0`. The simplest example is the rational numbers (fractions). For every
`p : ℚ` with `p ≠ 0` there is `p^{-1} : ℚ` such that `p · p^{-1} = 1` and
`p^{-1} · p = 1`. For a fraction `(a/b) : ℚ` the multiplicative inverse is
`(a/b)^{-1} = b/a`.

The real numbers include numbers like `√2 : ℝ` and `π : ℝ`, and have the
additional property that any convergent infinite sequence has a unique limit.
The complex numbers include `i = √(-1) : ℂ` and have the additional feature
that every polynomial equation has a solution (for example `x^2 + 1 = 0`);
this is called *algebraic completeness*.

# Ordering the numbers

Next we look at the relation `≤`, which defines a *partial order* on the
natural numbers. This time we are not going to use Lean's definition which uses an inductively defined relation (which we will cover later) but we define our own (equivalent) version.

We say that `m ≤ n` if there exists a number `k : ℕ` such
that `n = k + m`.

```anchor LE
def LE : Nat → Nat → Prop
| m , n => ∃ k : ℕ , k + m = n

infix:50 (priority := 1001) " ≤ " => LE
```
A *partial order* is a relation that is

- *Reflexive*: for all `x`, `x ≤ x`,
- *Transitive*: for all `x, y, z`, `x ≤ y` and `y ≤ 'z` imply `x ≤ z`,
- *Antisymmetric*: for all `x, y`, `x ≤ y` and `y ≤ x` imply `x = y`.

It is not hard to prove reflexivity and transitivity for the definition above
(reflexivity with `k = 0`, transitivity by adding the “differences” and using
associativity).

```anchor refl_LE
theorem refl_LE : ∀ n : ℕ, n ≤ n := by
intro n
exists 0
apply add_lneutr
```

```anchor trans_LE
theorem trans_LE : ∀ l m n : ℕ, l ≤ m → m ≤ n → l ≤ n := by
intro l m n
intro lm mn
cases lm with
| intro k klm =>
    cases mn with
    | intro j jmn =>
        exists j + k
        calc j + k + l
             = j + (k + l) := by rw [add_assoc]
            _ = j + m := by rw [klm]
            _ = n := by assumption
```


Antisymmetry takes a bit more work and benefits from a few
auxilliary lemmas — a good exercise.

```anchor anti_sym_LE
theorem anti_sym_LE : ∀ l m : ℕ , l ≤ m → m ≤ l → m = l := by
sorry
```

We can define `<` by `m < n` iff `m + 1 ≤ n`.

```anchor LT
def LT : ℕ → ℕ → Prop
| m , n => m+1 ≤ n

infix:50 (priority := 1001) " < " => LT
```

I will not discuss `<` in detail here: it is antireflexive
(*for all* `n`, *not* `n < n`), transitive, and strictly antisymmetric
(`m < n` and `n < m` implies `False`). Two key properties of `<` are:

- *Well-foundedness*: any strictly descending chain (for example
  `10 > 5 > 3 > 0`) eventually terminates. This property underpins proofs of
  termination for algorithms that are not primitive recursive.
- *Trichotomy*: for any two numbers `m, n`, exactly one of `m < n`,
  `m = n`, or `n < m` holds.

A relation that is transitive, trichotomous, and well-founded is called a
*well-order*. Well-orders are very useful; there is a famous theorem by
Cantor that every set can be well-ordered. This is equivalent to the axiom of choice and not accepted in intuitionism (it implies excluded middle), and no explicit well-ordering of
the real numbers is known.

# Decidability

Equality for natural numbers is *decidable*: there is an algorithm that, given
`m, n : ℕ`, determines whether `m = n`. This can be implemented as a function:
```anchor eq_ℕ
def eq_ℕ : ℕ → ℕ → Bool
| zero , zero => true
| zero , succ n => false
| succ m , zero => false
| succ m , succ n => eq_ℕ m n
```

We claim that `eq_ℕ` decides equality that is `m = n ↔ eq_ℕ m n = true` for any `m n : ℕ`. We are now going to prove this. To show the left to right direction it is sufficient to show that `eq_ℕ` is reflexive.

```anchor refl_eq_ℕ
theorem refl_eq_ℕ : ∀ n : ℕ, eq_ℕ n n = true := by
intro n
induction n with
| zero => rfl
| succ n ih => calc
    eq_ℕ (n + 1) (n + 1) =
    eq_ℕ n n := by rfl
    _ = true := by rw [ih]
```

Using rewriting this implies the left to right implication:
```anchor equal2eq
theorem equal2eq : ∀ m n : ℕ, m = n → eq_ℕ m n = true := by
intro m n mn
calc
  eq_ℕ m n
  = eq_ℕ m m := by rw [mn]
  _ = true := by rw [refl_eq_ℕ]
```

The left to right direction is more interesting. A first attempt fails:

```anchor eq2equal_bad
example : ∀ m n : ℕ, eq_ℕ m n = true → m = n := by
intro m n mn
induction m with
| zero =>
    cases n with
    | zero => rfl
    | succ n' => cases mn
| succ m' ih =>
    cases n with
    | zero => cases mn
    | succ n' => sorry
```

We get into the following state
```
m' n' : ℕ
ih : eq_ℕ m' (n' + 1) = true → m' = n' + 1
mn : eq_ℕ (m' + 1) (n' + 1) = true
⊢ m' + 1 = n' + 1
```

We need to apply `ih` for `n'` and not `n' + 1`. this is a common situation with induction proofs we have to make sure that our induction hypothesis is general enough. In this case this can be easily achieved by delaying the `intro`s so that the induction hypothesis applies to all `n`.
```anchor eq2equal
theorem eq2equal : ∀ m n : ℕ, eq_ℕ m n = true → m = n := by
intro m
induction m with
| zero =>
    intro n mn
    cases n with
    | zero => rfl
    | succ n' => cases mn
| succ m' ih =>
    intro n mn
    cases n with
    | zero => cases mn
    | succ n' =>
        have h : m' = n' := by
          apply ih
          calc
            eq_ℕ m' n'
              = eq_ℕ (succ m') (succ n') := by rfl
            _ = true := by rw [mn]
        rw [h]
```

Now we can package everything up and prove that `eq_ℕ` decides equality:
```anchor dec_eq_ℕ
theorem dec_eq_ℕ : ∀ m n : ℕ, m = n ↔ eq_ℕ m n = true := by
intro m n
constructor
. apply equal2eq
. apply eq2equal
```

We say that equality of natural numbers is *decidable*. Not every predicate or relation is decidable, a famous example which we will see in the next semester is the *Halting problem*. However, there are simpler examples, eg equality on functions on natural numbers cannot be decided. A positive example is the relation `≤` which is decidable. I leave this as an exercise.

```anchor dec_LE_ℕ
def le_ℕ : ℕ → ℕ → Bool
:= sorry

theorem dec_LE_ℕ : ∀ m n : ℕ, m ≤ n ↔ le_ℕ m n = true
:= by sorry
```
